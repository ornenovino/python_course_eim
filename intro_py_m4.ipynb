{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Tszo5zPsEd_f",
        "03nRbWm3EjeN",
        "zPiTx3OMGYJG"
      ],
      "mount_file_id": "1Zmrs0L9kTG5fqNrqd2EMwi828ZPzUMdG",
      "authorship_tag": "ABX9TyNV8RXRhhFWZjIqkiWwfFPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ornenovino/python_course_eim/blob/main/intro_py_m4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción a la programación en python"
      ],
      "metadata": {
        "id": "yjqZKO6FB1cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Módulo 4: Análisis de Texto y Webscrapping"
      ],
      "metadata": {
        "id": "S3j_4KLTB3fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducción al análisis de texto y cómo utilizar Python para realizar webscrapping."
      ],
      "metadata": {
        "id": "qjaQya-1B_iB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este módulo utilizaremos dos librerías nuevas `requests` para realizar peticiones del tipo `GET` a un servidor donde va a estar alojada una página web y `BeautifulSoup` que nos permite acceder a los distintos `<tags>` o elementos `HTML`."
      ],
      "metadata": {
        "id": "Ru3pRX8FDTge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "I-fC8_giB6Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrapping simple"
      ],
      "metadata": {
        "id": "Tszo5zPsEd_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para comenzar de manera simple tomamos un URL que contiene una tabla \n",
        "# de peliculas alojada en IMBD\n",
        "\n",
        "url = \"https://www.imdb.com/search/title/?groups=top_1000&ref_=adv_prv\" # colocamos el URL en una variable\n",
        "page = requests.get(url) # hacemos un request de tipo GET al servidor\n",
        "soup = BeautifulSoup(page.content, 'html.parser') # salvamos el html.parser que nos permitira acceder a los elementos del maquetado web\n",
        "\n",
        "# con un simple comando podemos acceder y guardar los títulos de las peliculas\n",
        "# sabiendo que los mismos son h3\n",
        "movies = soup.find_all('h3', class_='lister-item-header')\n",
        "movies"
      ],
      "metadata": {
        "id": "ei8mIzkgDk0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una lista vacia\n",
        "movie_data = []\n",
        "\n",
        "for movie in movies:  # hacemos una estructura de control que itere sobre la lista anterior movies\n",
        "\n",
        "    # nos quedamos con el titulo y lo guardamos\n",
        "    title = movie.find('a').text\n",
        "\n",
        "    # nos quedamos con el año y lo guardamos\n",
        "    year = movie.find('span', class_='lister-item-year text-muted unbold').text\n",
        "\n",
        "    # hacemos un append de ambos valores a nuestra lista inicial\n",
        "    movie_data.append((title, year))\n",
        "\n",
        "# hacemos un print de lo que se salvo en la lista\n",
        "print(movie_data)"
      ],
      "metadata": {
        "id": "Ts3iUf1PEBzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrapping complejo y dataframes"
      ],
      "metadata": {
        "id": "03nRbWm3EjeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importamos pandas\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oQeGF6JwEnTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sobreescribimos variables de interes\n",
        "page = requests.get(\"https://www.imdb.com/chart/top\")\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "# creamos la lista vacia\n",
        "movies_list = []"
      ],
      "metadata": {
        "id": "pyysHGdxEqlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encontramos la tabla en la pagina web\n",
        "tbody = soup.find('tbody', class_='lister-list')\n",
        "trs = tbody.find_all('tr')  # encontramos todas las filas\n",
        "\n",
        "for tr in trs:  # iteramos por sobre todas las filas\n",
        "\n",
        "    # nos quedamos con el titulo y el año de cada pelicula\n",
        "    title = tr.find('td', class_='titleColumn').a.text\n",
        "    year = tr.find('td', class_='titleColumn').span.text[1:-1]\n",
        "\n",
        "    # guardamos el rating\n",
        "    rating = float(tr.find('td', class_='ratingColumn imdbRating').strong.text)\n",
        "\n",
        "    # hacemos append de cada una de estas variables\n",
        "    movies_list.append((title, year, rating))"
      ],
      "metadata": {
        "id": "AgUcMbj8FBwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos un pandas dataframe de la lista anterior definiendo las columnas\n",
        "movies_df = pd.DataFrame(movies_list, columns=['Title', 'Year', 'Rating'])\n",
        "\n",
        "# incluso lo podemos salvar como un CSV\n",
        "movies_df.to_csv('imdb-movies-dataset.csv', index = False)"
      ],
      "metadata": {
        "id": "5EooIXC_FW7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos echar un vistazo del df que acabamos de crear\n",
        "movies_df.head()"
      ],
      "metadata": {
        "id": "j2i4EEdrFcCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análisis de texto"
      ],
      "metadata": {
        "id": "68wzG9waIOm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nube de palabras**"
      ],
      "metadata": {
        "id": "6hyZmlwAOqDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importamos las librerias\n",
        "import nltk \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ed_sheeran =  pd.read_csv('/content/drive/MyDrive/python_intro/EdSheeran.csv', encoding=\"utf-8\")\n",
        "ed_sheeran.head()"
      ],
      "metadata": {
        "id": "vWQ3xCC1ISLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos a tomar la letra de la cancion Shape of You\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "shape_of_you = ed_sheeran[ed_sheeran['Title'] == 'Shape of You']\n",
        "lyrics = shape_of_you['Lyric'].to_string()\n",
        "type(lyrics)"
      ],
      "metadata": {
        "id": "QAevrVO6I6xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing la letra por palabra\n",
        "tokens = word_tokenize(lyrics)\n",
        "\n",
        "# creamos una distrbucion simple de las palabras\n",
        "freq_dist = nltk.FreqDist(tokens) \n",
        "  \n",
        "# mostramos las 5 palabras más frecuentes en la letra\n",
        "print(freq_dist.most_common(5))"
      ],
      "metadata": {
        "id": "WNj8U7jOJG7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como notamos la letra contiene un montón de conectores que se deberían de eliminar."
      ],
      "metadata": {
        "id": "AUEWghC3NFbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# removemos puntuacion\n",
        "remove_punctuation = [char for char in lyrics if char not in string.punctuation] \n",
        "\n",
        "# agregamos de nuevo a la lista sin puntuacion\n",
        "remove_punctuation = ''.join(remove_punctuation) \n",
        "\n",
        "# removemos las conocidas 'stopwords'\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# hacemos la tokenizacion nuevamente\n",
        "word_tokens = word_tokenize(remove_punctuation) \n",
        "\n",
        "# realizamos un condicional como list comprehension\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]"
      ],
      "metadata": {
        "id": "z1XWOwV1NKB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))\n",
        "print(len(filtered_sentence)) # redujimos hasta la mitad las palabras\n",
        "filtered_sentence"
      ],
      "metadata": {
        "id": "PmdO_ZUPNiC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_clean = nltk.FreqDist(filtered_sentence)\n",
        "print(freq_dist_clean.most_common(5)) # cambian completamente las palabras mas frecuentes"
      ],
      "metadata": {
        "id": "xrb5TX_aN3Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos generar una nube de palabras\n",
        "wordcloud = WordCloud(width = 800, \n",
        "                      height = 800,\n",
        "                      background_color ='white', \n",
        "                      min_font_size = 10).generate(\" \".join(filtered_sentence)) \n",
        "\n",
        "# realizamos un plot de la misma\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9xPz_7PJOHeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentimientos**"
      ],
      "metadata": {
        "id": "NALV3S9jOtRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "n-jjC3ncOwBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_to_string = \" \".join(filtered_sentence)\n",
        "filtered_to_string"
      ],
      "metadata": {
        "id": "hygQz3gLSKcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos una función para analizar los sentimientos\n",
        "\n",
        "def sentiment_scores(sentence):\n",
        " \n",
        "    # creamos un objeto del tipo SentimentIntensityAnalyze\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        " \n",
        "    # usamos el método polarity_scores del SentimentIntensityAnalyzer\n",
        "    # que nos da un diccionario de sentimientos\n",
        "    # que contiene pos (positivos) neg (negativos) neu(neutros) y los el score\n",
        "    # el compound es la suma de los positivos, negativos y neutrales\n",
        "    # normalizado entre -1 extremo negativo y +1 extremo positivo\n",
        "    \n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "     \n",
        "    print(\"Los scores son: \", sentiment_dict)\n",
        "    print(\"La letra fue un \", sentiment_dict['neg']*100, \"% Negativa\")\n",
        "    print(\"La letra fue un  \", sentiment_dict['neu']*100, \"% Neutral\")\n",
        "    print(\"La letra fue un  \", sentiment_dict['pos']*100, \"% Positiva\")\n",
        " \n",
        "    print(\"Entonces la canción es\", end = \" \")\n",
        " \n",
        "    # imprimimos el resultado de acuerdo a donde caiga el número\n",
        "    if sentiment_dict['compound'] >= 0.05 :\n",
        "        print(\"Positiva\")\n",
        " \n",
        "    elif sentiment_dict['compound'] <= - 0.05 :\n",
        "        print(\"Negativa\")\n",
        " \n",
        "    else :\n",
        "        print(\"Neutral\")"
      ],
      "metadata": {
        "id": "CJqM_Tt9O3Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_scores(filtered_to_string)"
      ],
      "metadata": {
        "id": "mWseuKv3VJzN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}